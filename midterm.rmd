---
title: "CPLN 675 Midterm"
author: "Marquise Williams and Will Friedrichs"
date: "3/29/2022"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    runtime: shiny
---

For this project, a generalized linear model will be used to predict flood inundation zones first in Calgary, then in Sacramento County. For each area, predictions will estimate whether or not a given 500m by 500m "cell" of area can be described as in a flood inundation zone. This prediction process is described in four steps: data gathering, building variables, logistic metrics, and goodness of fit metrics.

```{r setup, include=TRUE,message = FALSE,cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
sf::sf_use_s2(FALSE)
options(scipen=999)
library(caret)
library(pscl)
library(plotROC)
library(pROC)
library(sf)
library(tidyverse)
library(knitr)
library(kableExtra)
library(tigris)
library(viridis)
```


### 1) Data Gathering

The following factors will be examined as predictors of Calgary's flood inundation zones.

<code>cal_dem</code>, <code>sac_dem</code>: For Calgary and Sacramento County respectively, <code>cal_dem</code> and <code>sac_dem</code> represent elevation data relative to the lowest point in the area. Flood inundation is probably more likely in low-lying areas than high elevation areas, so there is reason to suspect this factor might be significant.

<code>cal_streamdist</code>, <code>sac_streamdist</code>: For Calgary and Sacramento County respectively, <code>cal_dem</code> and <code>sac_dem</code> describe the distance between any cell and a stream. This is due to our hypothesis that areas with streams have a higher flooding suceptibility. 

NEED LAND USE, NEED TREE COVER, NEED DISTANCE TO RIVER AS ADDITIONAL FACTORS HERE

<code>cal_inundation</code>: This is what our model aims to predict. We will train our model on part of Calgary flood inundation fishnet, and test our predictions on another part of it.


```{r load_json, warning = FALSE, message = FALSE}
cal_dem <- st_read("https://raw.githubusercontent.com/Willfriedrichs/Midterm_Repo/main/Finalized_Layers/Cal_dem_filled.json")

cal_streamdist <- st_read("https://raw.githubusercontent.com/Willfriedrichs/Midterm_Repo/main/Finalized_Layers/cal_dist2streams.json")

cal_inundation <- st_read("https://raw.githubusercontent.com/Willfriedrichs/Midterm_Repo/main/Finalized_Layers/calgary_inundation.json")

cal_inundation[is.na(cal_inundation)] = 0

sac_dem <- st_read("https://raw.githubusercontent.com/Willfriedrichs/Midterm_Repo/main/Finalized_Layers/sac_dem_filled.geojson")

sac_streamdist <- st_read("https://raw.githubusercontent.com/Willfriedrichs/Midterm_Repo/main/Finalized_Layers/sac_dist2streams.geojson")
```


### 2) Building Variables and Running Model


In this step, the variables are joined together to form one table that includes each of the independent variables and the dependent variable. Variables will also be adjusted for better comparison across regions.

```{r load_chesco, warning = FALSE, message = FALSE}
# cal_dem only needs its geometry and information as to the mean elevation of each cell.
# mean elevation will be re-evaluated relative to minimum non-zero fishnet cell
cal_dem = cal_dem %>% select(ZonalSt_Fishnet1.MEAN)
min_nonzero_cell = min(cal_dem$ZonalSt_Fishnet1.MEAN)
cal_dem$ZonalSt_Fishnet1.MEAN = cal_dem$ZonalSt_Fishnet1.MEAN - min_nonzero_cell


#cal_streamdist only needs its geometry and the mean distance to stream from any given cell.
cal_streamdist = cal_streamdist %>% select(dist2streams.MEAN)

#cal_inundation only needs its geometry and the majority inundation for cells
cal_inundation = cal_inundation %>% select(calgary_inundation.MAJORITY)


# sac_dem only needs its geometry and information as to the mean elevation of each cell.
# since the minimum mean elevation is nonzero in Sacramento, no readjustment is necessary.
sac_dem = sac_dem %>% select(demFilledSAC.MEAN)

#sac_streamdist only needs its geometry and the mean distance to stream from any given cell.
sac_streamdist = sac_streamdist %>% select(dist2streamsSAC.MEAN)

# ADDITIONAL VARIABLES NEED TO BE ADDED HERE

# Once in a single dataframe, discard OIDs and rename variables
allCalgary = st_join(cal_dem, cal_streamdist, largest = TRUE) %>%
             st_join(cal_inundation, largest = TRUE)


allCalgary = allCalgary %>% rename(meanElevation = ZonalSt_Fishnet1.MEAN,
                                   meanStreamDistance = dist2streams.MEAN,
                                   inundation = calgary_inundation.MAJORITY)

ggplot() +
  geom_sf(data=allCalgary, aes(fill=as.factor(inundation))) +
  labs(title="cal_inundation")
```

The variables are now ready to be split into training and test sets, and finally, added to the linear model. The <code>glm()</code> takes in the independent variables and dependent from a portion of the dataset and produces a formula that minimizes prediction error. The formula's performance predicting the dependent variables in the test set can be used to characterize the performance of the model in Calgary.

The <code>set.seed()</code> and <code>createDataPartition</code> functions help to randomly select a portion (in this case 70% of cells) to join the training set. The rest form the test set.

```{r training_set}
set.seed(3456)
trainIndex <- createDataPartition(preserve$inundation, p = .70,
                                  list = FALSE,
                                  times = 1)

preserveTrain <- preserve[ trainIndex,]
preserveTest  <- preserve[-trainIndex,]
```
                     
              
### 3) logistic metrics

Below, the five independent fishnet variables are used to create a generalized linear model.

Now let’s estimate a logistic regression model. The binomial logit model runs in the `glm` function (generalized linear models). We specify the dependent variable as `preserve` and run the model on our training set `preserveTrain`.

Note how we can use the dplyr pipes right in the data parameter. We have to convert to a data frame because R won’t know how to run a regression on an sf.

Let's look at the model output, we see that we have coefficients, and p-values, but no R-squared. There are other goodness of fit metrics we will look at. The AIC, though not on a 0-1 scale like R-squared, has a similar function in that it tells you about overall model fit, but not about error and accuracy.

We are not really interested in our coefficients other than their magnitude, directionality and p-value (generall). But for the record, the way the coefficients in a logistic regression are interpreted is different than in OLS - we are talking in terms of "odds" of an outcome occurring (in our case odds of land being preserved.). If we exponentiate the coefficient (`exp()`) we can interpret it as *all else equal* the exponentiated value being the increase or decrease in the odds of the outcome.




### 4) Goodness of fit metrics


